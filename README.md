# 3D Tooth Segmentation using nnU-Net

A deep learning project for 3D segmentation of tooth structures from micro-CT (ÂµCT) scans using the nnU-Net framework. This project implements a complete pipeline for automated segmentation of dental tissues including enamel, dentin, and pulp.

## ğŸ¯ Project Overview

This project focuses on semantic segmentation of tooth structures in 3D CT volumes. The system uses nnU-Net, a self-configuring framework for biomedical image segmentation, to automatically segment three main dental tissue classes:

- **Enamel**: The outer protective layer of the tooth
- **Dentin**: The main hard tissue component
- **Pulp**: The inner soft tissue containing nerves and blood vessels

The project includes a comprehensive hyperparameter optimization (HPO) pipeline using Optuna to systematically explore parameter spaces and identify optimal configurations.

## âœ¨ Key Features

- ğŸ”¬ **nnU-Net Framework**: Leverages the state-of-the-art nnU-Net architecture for biomedical segmentation
- ğŸ›ï¸ **Hyperparameter Optimization**: Automated HPO pipeline with Optuna for systematic parameter search
- ğŸ“Š **Comprehensive Evaluation**: Multiple metrics including Dice Score, IoU, and Hausdorff Distance
- ğŸ† **Best Model Selection**: Automated identification and preparation of top-performing models
- ğŸ”„ **Reproducible Workflow**: Complete pipeline from preprocessing to evaluation with full traceability
- ğŸ“ˆ **Performance Tracking**: Detailed logging and analysis of all trials and experiments

## ğŸ“ Project Structure

```
CT-Tooth-Segmentation-DeepLearning/
â”œâ”€â”€ hpo/                          # Hyperparameter Optimization
â”‚   â”œâ”€â”€ scripts/                  # HPO pipeline scripts
â”‚   â”‚   â”œâ”€â”€ preprocessing/       # Preprocessing and trial generation
â”‚   â”‚   â”œâ”€â”€ training/            # Training and evaluation pipeline
â”‚   â”‚   â”œâ”€â”€ analysis/            # Model comparison and analysis
â”‚   â”‚   â””â”€â”€ utils/               # Utility scripts
â”‚   â”œâ”€â”€ config/                  # Configuration templates
â”‚   â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ best_model/              # Best performing model (trial_8, Dice: 0.9725)
â”‚   â”œâ”€â”€ preprocessing_output/    # Preprocessed datasets per trial
â”‚   â”œâ”€â”€ training_output/         # Trained models and checkpoints
â”‚   â””â”€â”€ results/                 # Evaluation results
â”œâ”€â”€ scripts/                      # Main project scripts
â”‚   â””â”€â”€ nnunet_env.sh         # Environment setup
â”œâ”€â”€ data/                        # Data directory (not in repo)
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for exploration
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended)
- nnU-Net v2 installed
- Conda environment (see `environment.yml`)

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd CT-Tooth-Segmentation-DeepLearning
   ```

2. **Set up environment**
   ```bash
   conda env create -f environment.yml
   conda activate <env-name>
   source scripts/nnunet_env.sh
   ```

3. **Prepare your data**
   - Organize your CT scans following nnU-Net conventions
   - Place data in the directory specified by `nnUNet_raw` environment variable

## ğŸ“– Usage

### Hyperparameter Optimization

The project includes a comprehensive HPO pipeline to systematically explore parameter spaces:

```bash
# Generate HPO trials
python hpo/scripts/preprocessing/nnunet_hpo_preprocess.py --n_trials 50

# Train and evaluate trials
python hpo/scripts/training/nnunet_train_eval_pipeline.py --folds 0

# Prepare best model for deployment
python hpo/scripts/analysis/prepare_best_model.py
```

For detailed HPO documentation, see [`hpo/README.md`](hpo/README.md).

### Training a Single Model

```bash
# Standard nnU-Net training
nnUNetv2_train <DATASET_ID> <CONFIG> <FOLD> -tr nnUNetTrainer -p nnUNetPlans
```

### Evaluation

```bash
# Evaluate trained model
nnUNetv2_evaluate -d <DATASET_ID> -c <CONFIG> -tr nnUNetTrainer -p nnUNetPlans -f <FOLD>
```

## ğŸ“Š Results

### Best Model Performance

After running 10 HPO trials, the best performing model achieved:

- **Trial**: `trial_8`
- **Dice Score**: **0.9725**
- **Configuration**:
  - Patch Size: [160, 160, 64]
  - Batch Size: 4
  - Features Base: 24
  - Batch Dice: False
  - Use Mask for Norm: False

### Top 3 Trials

| Rank | Trial | Dice Score | Key Parameters |
|------|-------|------------|----------------|
| 1 | trial_8 | 0.9725 | Patch: [160,160,64], Batch: 4 |
| 2 | trial_1 | 0.9723 | Patch: [128,64,128], Batch: 2 |
| 3 | trial_3 | 0.9721 | Patch: [64,128,64], Batch: 2 |

For detailed parameter analysis, see [`hpo/docs/BEST_PARAMETERS_SUMMARY.md`](hpo/docs/BEST_PARAMETERS_SUMMARY.md).

## ğŸ”§ Configuration

### Environment Variables

Set the following environment variables (via `scripts/nnunet_env.sh`):

- `nnUNet_raw`: Path to raw dataset directory
- `nnUNet_preprocessed`: Path to preprocessed data
- `nnUNet_results`: Path to training results

### Dataset Format

The project follows nnU-Net conventions:

```
nnUNet_raw/
â””â”€â”€ Dataset001_GroundTruth/
    â”œâ”€â”€ imagesTr/          # Training images
    â”œâ”€â”€ labelsTr/          # Training labels
    â”œâ”€â”€ imagesTs/          # Test images (optional)
    â””â”€â”€ dataset.json       # Dataset metadata
```

## ğŸ“ˆ Evaluation Metrics

The system computes the following metrics:

- **Dice Score**: Overlap measure between prediction and ground truth
- **IoU (Intersection over Union)**: Jaccard index
- **Hausdorff Distance**: Maximum surface distance
- **Volume Similarity**: Volume overlap ratio
- **Precision & Recall**: Classification metrics

## ğŸ› ï¸ Development

### Project Components

- **HPO Pipeline**: Automated hyperparameter search using Optuna
- **Training Pipeline**: Automated training and evaluation workflow
- **Analysis Tools**: Model comparison and best model selection
- **Utility Scripts**: Data validation, label remapping, and preprocessing fixes

### Key Scripts

- `hpo/scripts/preprocessing/nnunet_hpo_preprocess.py`: Generate HPO trials
- `hpo/scripts/training/nnunet_train_eval_pipeline.py`: Train and evaluate models
- `hpo/scripts/analysis/prepare_best_model.py`: Prepare best model for deployment
- `hpo/scripts/utils/`: Various utility scripts for data management

## ğŸ“š Documentation

- **[HPO Runbook](hpo/README.md)**: Complete guide for hyperparameter optimization
- **[Best Parameters Summary](hpo/docs/BEST_PARAMETERS_SUMMARY.md)**: Analysis of top-performing configurations
- **[Best Model Documentation](hpo/best_model/README.md)**: Instructions for using the best model

## ğŸ“ Citation

If you use this project in your research, please cite:

```bibtex
@software{tooth_segmentation_2025,
  author = {Johannes},
  title = {3D Tooth Segmentation using nnU-Net},
  year = {2025},
  url = {https://github.com/jqhnnes/Segmentierung-von-CT-Aufnahmen-extrahierter-Z-hne-mittels-Deep-Learning}
}
```

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.

---

**Note**: This project uses nnU-Net v2. For more information about nnU-Net, visit the [official repository](https://github.com/MIC-DKFZ/nnUNet).
